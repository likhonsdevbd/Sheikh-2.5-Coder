# Sheikh-2.5-Coder GitHub Actions Training Pipeline

## Overview

This repository contains a comprehensive GitHub Actions automated training pipeline for fine-tuning the Sheikh-2.5-Coder language model. The pipeline provides end-to-end automation from data preparation to model deployment.

## ğŸš€ Features

- **Automated Fine-tuning**: Multi-GPU distributed training with cloud infrastructure
- **Multi-stage Pipeline**: Data prep â†’ Training â†’ Evaluation â†’ Deployment
- **Flexible Triggers**: Scheduled runs, manual triggers, and push-based workflows
- **Intelligent Caching**: Dataset and model checkpoint caching for efficiency
- **Auto-deployment**: Automatic upload to HuggingFace Hub and GitHub repository
- **Comprehensive Evaluation**: MMLU, HumanEval, and web development benchmarks
- **Model Optimization**: INT8/INT4 quantization and memory optimization

## ğŸ“ File Structure

```
Sheikh-2.5-Coder/
â”œâ”€â”€ .github/
â”‚   â””â”€â”€ workflows/
â”‚       â”œâ”€â”€ train.yml          # Main training workflow
â”‚       â””â”€â”€ deploy.yml         # Deployment workflow
â”œâ”€â”€ scripts/
â”‚   â”œâ”€â”€ auto_train.py          # Automated training script
â”‚   â”œâ”€â”€ evaluate_model.py      # Comprehensive evaluation
â”‚   â”œâ”€â”€ deploy_model.py        # Automated deployment
â”‚   â””â”€â”€ setup_training_env.py  # Environment setup
â”œâ”€â”€ configs/
â”‚   â”œâ”€â”€ data_prep_config.yaml
â”‚   â””â”€â”€ training_config.json
â”œâ”€â”€ src/                       # Core training modules
â”œâ”€â”€ data/                      # Training datasets
â”œâ”€â”€ models/                    # Model checkpoints
â””â”€â”€ evaluation/                # Evaluation results
```

## ğŸ”§ Workflow Components

### 1. Training Workflow (`.github/workflows/train.yml`)

**Triggers:**
- Manual dispatch with configurable parameters
- Weekly scheduled runs (Sunday midnight)
- Push events to critical directories

**Stages:**
- **Setup**: Environment preparation and dependency installation
- **Data Preparation**: Stack v2 dataset processing and quality filtering
- **Training**: Distributed fine-tuning with LoRA optimization
- **Evaluation**: Comprehensive benchmark testing
- **Monitoring**: W&B integration for real-time tracking

**Configurable Parameters:**
- GPU type (A100, V100, T4)
- Training steps (default: 10,000)
- Batch size (default: 4)
- Learning rate (default: 2e-5)

### 2. Deployment Workflow (`.github/workflows/deploy.yml`)

**Features:**
- Automatic deployment after successful training
- Multiple quantization options (INT8, INT4)
- HuggingFace Hub integration
- GitHub repository updates
- Release notes generation

**Deployment Targets:**
- HuggingFace Model Hub
- GitHub Repository
- Local deployment packages

## ğŸ› ï¸ Core Scripts

### `auto_train.py`
Automated training script with:
- Distributed training support
- Mixed precision training
- Gradient checkpointing
- Real-time monitoring
- Automatic checkpoint saving
- LoRA fine-tuning optimization

### `evaluate_model.py`
Comprehensive evaluation framework:
- MMLU Code benchmark
- HumanEval coding tasks
- Web development specific tests
- Performance benchmarking
- Quality assessment metrics

### `deploy_model.py`
Deployment automation:
- Model quantization (INT8/INT4)
- Memory optimization
- ONNX export
- HuggingFace Hub upload
- Package creation

### `setup_training_env.py`
Environment preparation:
- Dependency management
- GPU configuration
- System diagnostics
- W&B setup
- HuggingFace configuration

## ğŸ¯ Usage

### Manual Training Trigger

Navigate to Actions tab and select "Automated Model Training":

1. **Choose GPU Type**: A100 (recommended), V100, or T4
2. **Set Training Parameters**:
   - Training steps: 10,000 (adjust based on needs)
   - Batch size: 4 (auto-adjusted for GPU)
   - Learning rate: 2e-5 (default)
3. **Run Workflow**: Monitor progress in real-time

### Quick Setup

```bash
# Setup training environment
python scripts/setup_training_env.py

# Quick test training
bash scripts/quick_train.sh

# Run evaluation
bash scripts/run_evaluation.sh

# Deploy model
bash scripts/deploy.sh
```

### Local Development

```bash
# Install dependencies
pip install -r requirements.txt

# Prepare data
python scripts/prepare_data.py

# Start training
python scripts/auto_train.py \
    --model_name microsoft/phi-2 \
    --data_path data/processed \
    --output_path models/checkpoints \
    --gpu_type t4 \
    --training_steps 1000
```

## ğŸ“Š Monitoring & Logging

### Weights & Biases Integration
- Real-time training metrics
- Loss curves and performance tracking
- Model artifact logging
- Experiment comparison

### GitHub Actions Logs
- Step-by-step execution logs
- Error handling and recovery
- Performance metrics
- Artifact management

### Custom Metrics
- Training loss and learning rate
- Evaluation accuracy scores
- Model size and performance
- Memory usage tracking

## ğŸ” Security & Secrets

Required GitHub Secrets:
- `HF_TOKEN`: HuggingFace API token
- `WANDB_API_KEY`: Weights & Biases API key
- `WANDB_ENTITY`: W&B team/entity name

Optional secrets:
- `GITHUB_TOKEN`: For repository operations (auto-provided)

## ğŸ“ˆ Performance Optimization

### GPU-Specific Configurations
- **A100**: 80GB memory, 4x batch size multiplier
- **V100**: 32GB memory, 2x batch size multiplier  
- **T4**: 16GB memory, 1x batch size multiplier

### Memory Optimizations
- Gradient checkpointing
- Mixed precision training (FP16)
- Dynamic batching
- Model quantization (INT8/INT4)

### Caching Strategy
- Dataset preprocessing cache
- Model checkpoint cache
- Dependency cache
- Environment cache

## ğŸ›ï¸ Configuration

### Training Configuration (`configs/training_config.json`)
```json
{
  "training": {
    "base_model": "microsoft/phi-2",
    "max_sequence_length": 2048,
    "learning_rate": 2e-05,
    "per_device_train_batch_size": 4,
    "gradient_accumulation_steps": 4
  },
  "lora": {
    "r": 16,
    "lora_alpha": 32,
    "lora_dropout": 0.1
  }
}
```

### Data Preparation (`configs/data_prep_config.yaml`)
```yaml
dataset_sources:
  - name: "stack_v2"
    url: "huggingface/datasets/bigcode/the-stack-v2"
    processing: "quality_filtered"
  
synthetic_data:
  - language: "python"
    count: 10000
  - language: "javascript"
    count: 5000

filters:
  min_quality_score: 0.7
  max_length: 2048
  remove_duplicates: true
```

## ğŸš¨ Troubleshooting

### Common Issues

1. **GPU Out of Memory**
   - Reduce batch size
   - Enable gradient checkpointing
   - Use model quantization

2. **Slow Training**
   - Check GPU utilization
   - Verify data pipeline efficiency
   - Monitor network I/O

3. **Evaluation Failures**
   - Check model checkpoint integrity
   - Verify evaluation dependencies
   - Review benchmark configurations

4. **Deployment Issues**
   - Validate HuggingFace credentials
   - Check model format compatibility
   - Verify quantization support

### Getting Help

- Check workflow logs for detailed error messages
- Review `logs/setup.log` for environment issues
- Monitor Weights & Biases dashboard for training metrics
- Consult GitHub Actions documentation

## ğŸ“ Development Guidelines

### Adding New Benchmarks
1. Create evaluation script in `evaluation/`
2. Add benchmark to `evaluate_model.py`
3. Update model card generation
4. Add required dependencies

### Custom Training Configurations
1. Modify `configs/training_config.json`
2. Update workflow parameters
3. Test with quick training run
4. Document changes

### Deployment Enhancements
1. Extend `deploy_model.py` with new formats
2. Add platform-specific optimizations
3. Update model card templates
4. Test deployment pipeline

## ğŸ“Š Current Performance

- **Training Speed**: ~100 tokens/second on A100
- **Memory Usage**: <16GB for base model with LoRA
- **Evaluation Coverage**: 4 comprehensive benchmarks
- **Deployment Time**: <5 minutes for quantization

## ğŸ¯ Roadmap

- [ ] Multi-model support (CodeLlama, StarCoder)
- [ ] Federated learning capabilities
- [ ] Advanced evaluation metrics
- [ ] Real-time inference API
- [ ] Model compression techniques
- [ ] Distributed evaluation pipeline

---

**Note**: This pipeline is designed for automated fine-tuning of code generation models. For custom requirements, modify the configuration files and workflows accordingly.