#!/usr/bin/env python3
"""
Automated Model Card Generator for Sheikh-2.5-Coder
Creates comprehensive model cards with performance metrics, usage examples, and documentation
"""

import os
import sys
import json
import argparse
import logging
import yaml
from datetime import datetime
from pathlib import Path
from typing import Dict, List, Optional, Union, Any
import subprocess
import tempfile

logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)


class ModelCardGenerator:
    """Automated model card generation with comprehensive documentation"""
    
    def __init__(self, model_name: str = "Sheikh-2.5-Coder", base_model: str = "microsoft/phi-2"):
        self.model_name = model_name
        self.base_model = base_model
        self.timestamp = datetime.now().strftime('%Y-%m-%d %H:%M:%S')
        
        logger.info(f"Initialized model card generator for {model_name}")
    
    def generate_complete_model_card(self, config: Dict, output_path: Optional[str] = None) -> str:
        """Generate comprehensive model card"""
        logger.info("Generating complete model card")
        
        card_sections = [
            self._generate_header_section(config),
            self._generate_model_overview_section(config),
            self._generate_usage_section(config),
            self._generate_training_details_section(config),
            self._generate_performance_section(config),
            self._generate_limitations_section(config),
            self._generate_evaluation_section(config),
            self._generate_examples_section(config),
            self._generate_citation_section(),
            self._generate_license_section()
        ]
        
        full_card = "\n\n".join(card_sections)
        
        if output_path:
            with open(output_path, 'w', encoding='utf-8') as f:
                f.write(full_card)
            logger.info(f"Model card saved to {output_path}")
        
        return full_card
    
    def generate_variant_model_card(self, variant_name: str, variant_config: Dict, 
                                   output_path: Optional[str] = None) -> str:
        """Generate model card for model variant (quantized, optimized, etc.)"""
        logger.info(f"Generating model card for variant: {variant_name}")
        
        card_sections = [
            self._generate_header_section(variant_config, variant_name),
            self._generate_variant_overview_section(variant_config, variant_name),
            self._generate_optimization_details_section(variant_config, variant_name),
            self._generate_usage_section(variant_config, variant_name),
            self._generate_performance_section(variant_config, variant_name),
            self._generate_comparison_section(variant_name),
            self._generate_limitations_section(variant_config, variant_name),
            self._generate_examples_section(variant_config, variant_name),
            self._generate_citation_section(variant_name),
            self._generate_license_section()
        ]
        
        full_card = "\n\n".join(card_sections)
        
        if output_path:
            with open(output_path, 'w', encoding='utf-8') as f:
                f.write(full_card)
            logger.info(f"Variant model card saved to {output_path}")
        
        return full_card
    
    def generate_release_model_card(self, release_info: Dict, output_path: Optional[str] = None) -> str:
        """Generate model card for specific release"""
        logger.info(f"Generating release model card: {release_info.get('version', 'unknown')}")
        
        card_sections = [
            self._generate_release_header_section(release_info),
            self._generate_whats_new_section(release_info),
            self._generate_changelog_section(release_info),
            self._generate_model_overview_section(release_info),
            self._generate_usage_section(release_info),
            self._generate_training_details_section(release_info),
            self._generate_performance_section(release_info),
            self._generate_evaluation_section(release_info),
            self._generate_examples_section(release_info),
            self._generate_compatibility_section(release_info),
            self._generate_upgrade_guide_section(release_info),
            self._generate_citation_section(release_info.get('version')),
            self._generate_license_section()
        ]
        
        full_card = "\n\n".join(card_sections)
        
        if output_path:
            with open(output_path, 'w', encoding='utf-8') as f:
                f.write(full_card)
            logger.info(f"Release model card saved to {output_path}")
        
        return full_card
    
    def _generate_header_section(self, config: Dict, variant_name: Optional[str] = None) -> str:
        """Generate model card header"""
        model_display_name = self.model_name
        if variant_name:
            model_display_name += f" - {variant_name.title()}"
        
        header = f"""# {model_display_name}

<div align="center">
  <img src="https://img.shields.io/badge/ðŸ¤—-Hugging%20Face-blue?style=for-the-badge&logo=huggingface" alt="Hugging Face">
  <img src="https://img.shields.io/badge/Python-3776AB?style=for-the-badge&logo=python&logoColor=white" alt="Python">
  <img src="https://img.shields.io/badge/License-MIT-green?style=for-the-badge" alt="License">
</div>

## Quick Links
- [Model Hub](https://huggingface.co/sheikh-team/{self.model_name.replace('-', '').lower()})
- [GitHub Repository](https://github.com/sheikh-team/{self.model_name.replace('-', '').lower()})
- [Documentation](https://github.com/sheikh-team/{self.model_name.replace('-', '').lower()}/docs)
- [Paper/Report](https://arxiv.org/abs/XXXX.XXXXX)
- [Demo](https://huggingface.co/spaces/sheikh-team/{self.model_name.replace('-', '').lower()}-demo)

## Model Information
"""
        
        return header
    
    def _generate_release_header_section(self, release_info: Dict) -> str:
        """Generate header for release-specific model card"""
        version = release_info.get('version', 'Unknown')
        release_name = release_info.get('release_name', version)
        
        header = f"""# {self.model_name} - Release {release_name}

<div align="center">
  <h3>ðŸŽ‰ Release {release_name}</h3>
  <p><strong>Released:</strong> {self.timestamp}</p>
  <img src="https://img.shields.io/badge/ðŸ¤—-Hugging%20Face-blue?style=for-the-badge&logo=huggingface" alt="Hugging Face">
  <img src="https://img.shields.io/badge/Version-{version}-orange?style=for-the-badge" alt="Version">
  <img src="https://img.shields.io/badge/License-MIT-green?style=for-the-badge" alt="License">
</div>

## Release {version} Highlights
{self._format_release_highlights(release_info)}

---

## Model Information
"""
        
        return header
    
    def _generate_variant_overview_section(self, config: Dict, variant_name: str) -> str:
        """Generate overview section for variant model"""
        variant_descriptions = {
            'int8': "8-bit quantized version optimized for reduced memory usage while maintaining excellent performance.",
            'int4': "4-bit quantized version providing maximum memory efficiency for deployment on resource-constrained systems.",
            'onnx': "ONNX format version optimized for cross-platform inference and acceleration on various hardware.",
            'tensorrt': "TensorRT optimized version for maximum inference speed on NVIDIA GPUs.",
            'coreml': "CoreML version for optimal performance on Apple devices (iOS/macOS).",
            'tflite': "TensorFlow Lite version for Android deployment and edge computing.",
            'gguf': "GGUF format optimized for CPU inference with minimal memory footprint."
        }
        
        description = variant_descriptions.get(variant_name, f"Optimized {variant_name} version of {self.model_name}.")
        
        section = f"""### Overview
{description}

**Key Features:**
- **Optimized Format**: {variant_name.upper()}
- **Target Platform**: {self._get_target_platform(variant_name)}
- **Memory Efficiency**: {self._get_memory_efficiency(variant_name)}
- **Speed Characteristics**: {self._get_speed_characteristics(variant_name)}
- **Quality Impact**: {self._get_quality_impact(variant_name)}

### Model Details
"""
        
        return section
    
    def _generate_model_overview_section(self, config: Dict) -> str:
        """Generate main model overview section"""
        model_size = config.get('model_size', 'Unknown')
        parameters = config.get('parameters', '2.7B')
        context_length = config.get('context_length', '2048')
        
        section = f"""- **Model Name**: {self.model_name}
- **Base Model**: {self.base_model}
- **Model Type**: Code Generation Language Model
- **Parameters**: {parameters} (trainable)
- **Context Length**: {context_length} tokens
- **Architecture**: Transformer-based Causal LM
- **Training Framework**: PyTorch + Transformers
- **Fine-tuning Method**: LoRA (Low-Rank Adaptation)
- **Model Format**: PyTorch (.safetensors)
- **Quantization**: {config.get('quantization', 'None')}
- **Deployment Ready**: âœ… Yes

### Intended Uses & Limitations

#### Intended Uses
- **Code Generation**: Generate high-quality Python, JavaScript, HTML/CSS, and other programming language code
- **Function Completion**: Complete partially written functions and code snippets
- **Documentation Generation**: Generate code comments and documentation
- **Bug Fixing**: Suggest fixes for common code issues
- **Code Refactoring**: Propose code improvements and optimizations
- **Learning Assistant**: Educational tool for programming students and developers

#### Limitations
- **Context Dependency**: Performance varies based on code context and quality
- **Language Specificity**: Best performance on Python, JavaScript, HTML/CSS
- **Code Safety**: Generated code should be reviewed before production use
- **Domain Knowledge**: Limited knowledge of highly specialized domains
- **Dependencies**: May not handle external library specifics perfectly
"""
        
        return section
    
    def _generate_whats_new_section(self, release_info: Dict) -> str:
        """Generate what's new section for release"""
        version = release_info.get('version', 'Unknown')
        highlights = release_info.get('highlights', [])
        
        highlights_text = "\n".join(f"- {highlight}" for highlight in highlights)
        if not highlights_text:
            highlights_text = "- Model improvements and optimizations\n- Documentation updates\n- Performance enhancements"
        
        section = f"""### ðŸŽ‰ What's New in Release {version}

{highlights_text}

#### Key Improvements
- **Performance**: {release_info.get('performance_improvement', 'Various optimizations applied')}
- **Model Size**: {release_info.get('model_size', 'Optimized for deployment')}
- **Documentation**: Enhanced usage examples and guides
- **Quality**: Improved code generation quality
"""
        
        return section
    
    def _generate_changelog_section(self, release_info: Dict) -> str:
        """Generate changelog section"""
        version = release_info.get('version', 'Unknown')
        changes = release_info.get('changes', [])
        
        changes_text = "\n".join(f"- {change}" for change in changes)
        if not changes_text:
            changes_text = "- Initial release features\n- Core functionality implementation\n- Documentation setup"
        
        section = f"""### ðŸ“‹ Changelog - Release {version}

{changes_text}

#### Technical Changes
- Model architecture optimizations
- Training pipeline improvements
- Quality assurance enhancements
- Deployment infrastructure updates
"""
        
        return section
    
    def _generate_optimization_details_section(self, config: Dict, variant_name: str) -> str:
        """Generate optimization details for variants"""
        optimization_info = self._get_optimization_details(variant_name)
        
        section = f"""### {variant_name.title()} Optimization Details

{optimization_info['description']}

#### Technical Specifications
"""
        
        for spec in optimization_info['specifications']:
            section += f"- {spec}\n"
        
        section += "\n#### Benefits\n"
        for benefit in optimization_info['benefits']:
            section += f"- {benefit}\n"
        
        section += "\n#### Considerations\n"
        for consideration in optimization_info['considerations']:
            section += f"- {consideration}\n"
        
        return section
    
    def _generate_usage_section(self, config: Dict, variant_name: Optional[str] = None) -> str:
        """Generate usage section"""
        model_identifier = f"sheikh-team/{self.model_name.replace('-', '').lower()}"
        if variant_name:
            model_identifier += f"-{variant_name}"
        
        section = f"""### Quick Start

#### Installation
```bash
pip install transformers torch accelerate
# For quantized versions:
pip install bitsandbytes
```

#### Basic Usage
```python
from transformers import AutoModelForCausalLM, AutoTokenizer
import torch

# Load model and tokenizer
model = AutoModelForCausalLM.from_pretrained(
    "{model_identifier}",
    torch_dtype=torch.float16,
    device_map="auto",
    trust_remote_code=True
)

tokenizer = AutoTokenizer.from_pretrained(
    "{model_identifier}",
    trust_remote_code=True
)

# Ensure pad token is set
if tokenizer.pad_token is None:
    tokenizer.pad_token = tokenizer.eos_token

# Generate code
prompt = "def fibonacci(n):"
inputs = tokenizer(prompt, return_tensors="pt")
outputs = model.generate(
    **inputs,
    max_new_tokens=100,
    temperature=0.7,
    do_sample=True,
    pad_token_id=tokenizer.eos_token_id
)

result = tokenizer.decode(outputs[0], skip_special_tokens=True)
print(result)
```

#### Advanced Usage with Quantization
```python
# For 8-bit quantization
from transformers import BitsAndBytesConfig

quantization_config = BitsAndBytesConfig(
    load_in_8bit=True,
    llm_int8_threshold=6.0,
)

model = AutoModelForCausalLM.from_pretrained(
    "{model_identifier}",
    quantization_config=quantization_config,
    device_map="auto",
    trust_remote_code=True
)

# For 4-bit quantization
quantization_config = BitsAndBytesConfig(
    load_in_4bit=True,
    bnb_4bit_compute_dtype=torch.bfloat16,
    bnb_4bit_use_double_quant=True,
    bnb_4bit_quant_type="nf4"
)

model = AutoModelForCausalLM.from_pretrained(
    "{model_identifier}",
    quantization_config=quantization_config,
    device_map="auto",
    trust_remote_code=True
)
```

#### Command Line Interface
```bash
# Basic code generation
python -c "
from transformers import AutoModelForCausalLM, AutoTokenizer

model = AutoModelForCausalLM.from_pretrained('{model_identifier}', torch_dtype=torch.float16, device_map='auto')
tokenizer = AutoTokenizer.from_pretrained('{model_identifier}')

prompt = 'Create a function to sort a list:'
inputs = tokenizer(prompt, return_tensors='pt')
outputs = model.generate(**inputs, max_new_tokens=50)
print(tokenizer.decode(outputs[0], skip_special_tokens=True))
"
```
"""
        
        return section
    
    def _generate_training_details_section(self, config: Dict) -> str:
        """Generate training details section"""
        section = f"""### Training Details

#### Dataset
- **Primary Dataset**: Stack v2 Code Dataset
- **Synthetic Data**: Custom generated code examples
- **Data Size**: {config.get('training_data_size', 'Large-scale code corpus')}
- **Languages**: Python, JavaScript, HTML/CSS, TypeScript, and more
- **Quality Filtering**: Multi-stage filtering for code quality and correctness

#### Training Configuration
- **Framework**: PyTorch + Transformers
- **Optimization**: AdamW optimizer
- **Learning Rate**: {config.get('learning_rate', '1e-4')} with warmup
- **Batch Size**: {config.get('batch_size', 'Dynamic batch sizing')}
- **Gradient Accumulation**: {config.get('gradient_accumulation', 'Enabled')}
- **Mixed Precision**: FP16 training
- **Hardware**: {config.get('hardware', 'Cloud GPU infrastructure')}

#### Fine-tuning Method
- **Base Method**: LoRA (Low-Rank Adaptation)
- **Rank**: {config.get('lora_rank', '16')}
- **Alpha**: {config.get('lora_alpha', '32')}
- **Dropout**: {config.get('lora_dropout', '0.1')}
- **Target Modules**: Query and Value matrices

#### Training Process
1. **Data Preparation**: Tokenization and quality filtering
2. **Model Loading**: Base model initialization with PEFT
3. **Training Loop**: Supervised fine-tuning with gradient checkpointing
4. **Validation**: Continuous evaluation on held-out dataset
5. **Checkpointing**: Regular model saves with best model selection
6. **Final Optimization**: Post-training quantization and optimization
"""
        
        return section
    
    def _generate_performance_section(self, config: Dict, variant_name: Optional[str] = None) -> str:
        """Generate performance metrics section"""
        section = f"""### Performance Metrics

#### Inference Speed
"""
        
        if variant_name:
            section += f"- **{variant_name.title()} Version**: Optimized for {self._get_target_platform(variant_name)}\n"
            section += f"- **Memory Usage**: {self._get_memory_efficiency(variant_name)} compared to base model\n"
            section += f"- **Inference Speed**: {self._get_speed_characteristics(variant_name)}\n"
        else:
            section += f"""- **Base Model**: High-performance inference on modern GPUs
- **Quantized Models**: Reduced memory usage with slight speed trade-offs
- **ONNX Version**: Cross-platform compatibility with hardware acceleration
"""

        section += f"""
#### Model Characteristics
- **Compression Ratio**: {config.get('compression_ratio', 'Variable by optimization type')}
- **Quality Retention**: {config.get('quality_retention', 'High quality maintained across variants')}
- **Memory Footprint**: {config.get('memory_footprint', 'Optimized for various deployment scenarios')}

#### Benchmark Results
"""
        
        if 'benchmark_results' in config:
            for benchmark, score in config['benchmark_results'].items():
                section += f"- **{benchmark}**: {score}\n"
        else:
            section += f"""- **Code Generation**: Quality evaluation ongoing
- **HumanEval**: Evaluation in progress
- **MMLU Code**: Benchmark results to be published
- **Performance Benchmarks**: Continuous evaluation ongoing
"""
        
        return section
    
    def _generate_limitations_section(self, config: Dict, variant_name: Optional[str] = None) -> str:
        """Generate limitations section"""
        base_limitations = """- **Code Quality**: Generated code should be reviewed before production use
- **Security**: May generate insecure code patterns - security review required
- **Dependencies**: Limited knowledge of specific library versions and APIs
- **Complex Logic**: May struggle with highly complex algorithmic implementations
- **Context Window**: Limited to model's context length for code understanding"""
        
        if variant_name:
            variant_limitations = f"""
#### {variant_name.title()} Version Specific
- **Platform Dependency**: Optimized for {self._get_target_platform(variant_name)}
- **Quality Trade-offs**: {self._get_quality_impact(variant_name)}
- **Hardware Requirements**: {self._get_hardware_requirements(variant_name)}"""
        else:
            variant_limitations = ""
        
        section = f"""### Limitations and Ethical Considerations

{base_limitations}{variant_limitations}

#### Recommendations for Use
1. **Code Review**: Always review generated code before deployment
2. **Testing**: Thoroughly test generated code in development environment
3. **Security Audit**: Perform security review for production use
4. **Version Control**: Track model versions for reproducible results
5. **Monitoring**: Monitor model performance and outputs in production

#### Responsible Use
- Use as a coding assistant, not a replacement for developer expertise
- Verify all generated code meets your project's standards
- Consider ethical implications of AI-generated code
- Respect intellectual property and licensing requirements
"""
        
        return section
    
    def _generate_evaluation_section(self, config: Dict) -> str:
        """Generate evaluation results section"""
        section = f"""### Evaluation Results

#### Automated Benchmarks
"""
        
        if 'evaluation_results' in config:
            for eval_name, eval_result in config['evaluation_results'].items():
                section += f"- **{eval_name}**: {eval_result}\n"
        else:
            section += f"""- **Code Generation Quality**: Comprehensive evaluation ongoing
- **Code Correctness**: Testing against known correct implementations
- **Code Style**: Evaluation of coding style and conventions
- **Performance**: Speed and memory usage benchmarks
"""
        
        section += f"""
#### Human Evaluation
- **Code Readability**: Human assessment of generated code clarity
- **Functionality**: Verification of code correctness and completeness
- **Best Practices**: Evaluation of coding best practices adherence
- **Documentation**: Assessment of generated comments and documentation

#### Comparison with Baselines
"""
        
        if 'baseline_comparisons' in config:
            for baseline, comparison in config['baseline_comparisons'].items():
                section += f"- **{baseline}**: {comparison}\n"
        else:
            section += f"""- **GPT-based Models**: Performance comparison in progress
- **Other Code Models**: Comparative analysis ongoing
- **Efficiency**: Memory and speed comparisons available
"""
        
        return section
    
    def _generate_examples_section(self, config: Dict, variant_name: Optional[str] = None) -> str:
        """Generate usage examples section"""
        section = """### Usage Examples

#### Python Code Generation
```python
# Generate a Python function
prompt = '''
def calculate_gcd(a, b):
    # Your code here
'''

# Generate the implementation
inputs = tokenizer(prompt, return_tensors="pt")
outputs = model.generate(**inputs, max_new_tokens=50, temperature=0.3)
result = tokenizer.decode(outputs[0], skip_special_tokens=True)
print(result)
```

#### JavaScript/React Components
```javascript
// Generate a React component
const prompt = `
function UserCard({ user }) {
  return (
    // Create a user card component
  );
}
`;

// Expected output includes JSX for user card
```

#### HTML/CSS Styling
```html
<!-- Generate responsive navigation -->
<nav class="navbar">
  <!-- Responsive navigation with CSS -->
</nav>
```

#### API Endpoint Generation
```python
# Generate FastAPI endpoint
prompt = '''
@app.post("/api/users")
async def create_user(user_data: UserCreate):
    # Create user endpoint
'''

# Expected output includes database integration, validation, etc.
```

#### Code Completion Examples
```python
# Function signature completion
def process_data(data_list: List[Dict], filter_criteria: Dict) -> List[Dict]:
    # Model completes the function body
    pass

# Class method completion
class DataProcessor:
    def __init__(self, config: dict):
        # Complete initialization
        pass
    
    def process(self, data):
        # Complete processing logic
        pass
```

#### Testing and Debugging
```python
# Generate unit tests
def test_calculate_fibonacci():
    # Generate test cases and assertions
    pass
```
"""
        
        if variant_name:
            section += f"\n#### {variant_name.title()} Specific Usage\n"
            section += self._get_variant_specific_examples(variant_name)
        
        return section
    
    def _generate_comparison_section(self, variant_name: str) -> str:
        """Generate comparison section for variants"""
        comparisons = {
            'int8': """| Aspect | Base Model | INT8 Quantized |
|---------|------------|----------------|
| Memory Usage | ~5.4GB | ~2.7GB (50% reduction) |
| Speed | Baseline | Slightly slower |
| Quality | Full precision | Minimal impact |
| GPU Memory | High | Reduced |
| CPU Usage | Moderate | Lower |""",
            
            'int4': """| Aspect | Base Model | INT4 Quantized |
|---------|------------|----------------|
| Memory Usage | ~5.4GB | ~1.35GB (75% reduction) |
| Speed | Baseline | Moderate slowdown |
| Quality | Full precision | Minor impact |
| GPU Memory | High | Very low |
| CPU Usage | Moderate | Lowest |""",
            
            'onnx': """| Aspect | Base Model | ONNX Optimized |
|---------|------------|----------------|
| Memory Usage | ~5.4GB | Variable by hardware |
| Speed | Baseline | Faster on supported HW |
| Quality | Full precision | Equivalent |
| Platform Support | PyTorch only | Cross-platform |
| Hardware Acceleration | Limited | Extensive |"""
        }
        
        comparison_table = comparisons.get(variant_name, "| Aspect | Comparison | Available")
        
        section = f"""### Performance Comparison

{comparison_table}

### When to Use This Variant
{self._get_variant_use_cases(variant_name)}
"""
        
        return section
    
    def _generate_compatibility_section(self, release_info: Dict) -> str:
        """Generate compatibility section for releases"""
        section = f"""### Compatibility

#### System Requirements
- **Python**: 3.8+
- **PyTorch**: 1.12+ (for base model)
- **Memory**: {release_info.get('memory_requirement', '4GB+ recommended')}
- **GPU**: Optional, for faster inference
- **Storage**: {release_info.get('storage_requirement', '1GB+ for model files')}

#### Platform Support
"""
        
        platforms = release_info.get('platforms', [
            'Linux (x86_64, ARM64)',
            'Windows (x86_64)',
            'macOS (Intel, Apple Silicon)',
            'Docker containers'
        ])
        
        for platform in platforms:
            section += f"- {platform}\n"
        
        section += f"""
#### Framework Compatibility
"""
        
        frameworks = release_info.get('frameworks', [
            'HuggingFace Transformers',
            'PyTorch',
            'ONNX Runtime',
            'TensorRT',
            'Core ML',
            'TensorFlow Lite'
        ])
        
        for framework in frameworks:
            section += f"- {framework}\n"
        
        return section
    
    def _generate_upgrade_guide_section(self, release_info: Dict) -> str:
        """Generate upgrade guide section"""
        version = release_info.get('version', 'Unknown')
        
        section = f"""### Upgrade Guide - Release {version}

#### What's New
- {self._format_release_highlights(release_info)}

#### Breaking Changes
{release_info.get('breaking_changes', 'No breaking changes in this release.')}

#### Migration Steps
1. **Backup Current Version**
   ```bash
   # Backup existing model files
   cp -r ~/.cache/huggingface/hub/* backup/
   ```

2. **Update Model**
   ```python
   # Clear cache and download new version
   from huggingface_hub import snapshot_download
   snapshot_download(
       repo_id="sheikh-team/{self.model_name.replace('-', '').lower()}",
       cache_dir="/tmp/hf-cache"
   )
   ```

3. **Verify Installation**
   ```python
   # Test new model
   from transformers import AutoModelForCausalLM
   model = AutoModelForCausalLM.from_pretrained("sheikh-team/{self.model_name.replace('-', '').lower()}")
   print("Model loaded successfully!")
   ```

#### Performance Considerations
- {release_info.get('performance_notes', 'Performance optimizations applied')}
- Model loading may take longer on first run due to cache updates
- Consider using model quantization for memory optimization
"""
        
        return section
    
    def _generate_citation_section(self, variant_name: Optional[str] = None) -> str:
        """Generate citation section"""
        model_ref = self.model_name.replace('-', '').lower()
        if variant_name:
            model_ref += f"-{variant_name}"
        
        section = f"""### Citation

If you use this model in your research or applications, please cite:

```bibtex
@software{{sheikh_2_5_coder_{model_ref},
  title={{{self.model_name}{' - ' + variant_name.title() if variant_name else ''}}},
  author={{Sheikh Development Team}},
  year={{2024}},
  url={{https://huggingface.co/sheikh-team/{self.model_name.replace('-', '').lower()}{'-' + variant_name if variant_name else ''}}},
  note={{Code generation language model optimized for programming tasks}}
}}
```

### Contact and Support

- **Issues**: [GitHub Issues](https://github.com/sheikh-team/{model_ref}/issues)
- **Discussions**: [GitHub Discussions](https://github.com/sheikh-team/{model_ref}/discussions)
- **Email**: sheikh-team@example.com
- **Documentation**: [Full Documentation](https://github.com/sheikh-team/{model_ref}/docs)

### Acknowledgments

- **Base Model**: microsoft/phi-2
- **Training Framework**: HuggingFace Transformers
- **Optimization**: PEFT library for parameter-efficient fine-tuning
- **Dataset**: Stack v2 Code Dataset
- **Infrastructure**: Cloud GPU providers for training resources

### License

This model is released under the MIT License. See the [LICENSE](LICENSE) file for details.
"""
        
        return section
    
    def _generate_license_section(self) -> str:
        """Generate license section"""
        section = """### License

#### MIT License

```
MIT License

Copyright (c) 2024 Sheikh Development Team

Permission is hereby granted, free of charge, to any person obtaining a copy
of this software and associated documentation files (the "Software"), to deal
in the Software without restriction, including without limitation the rights
to use, copy, modify, merge, publish, distribute, sublicense, and/or sell
copies of the Software, and to permit persons to whom the Software is
furnished to do so, subject to the following conditions:

The above copyright notice and this permission notice shall be included in all
copies or substantial portions of the Software.

THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE
AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,
OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE
SOFTWARE.
```

#### Commercial Use

This model can be used for commercial purposes without additional licensing fees.
Please ensure compliance with the terms of the base model license and any
applicable open source licenses of used dependencies.
"""
        
        return section
    
    def create_deployment_cards(self, config: Dict, output_dir: str) -> Dict:
        """Create multiple model cards for different deployment scenarios"""
        logger.info("Creating deployment-specific model cards")
        
        output_dir = Path(output_dir)
        output_dir.mkdir(parents=True, exist_ok=True)
        
        cards_created = {}
        
        # Base model card
        base_card = self.generate_complete_model_card(
            config,
            output_path=str(output_dir / "README.md")
        )
        cards_created['base'] = str(output_dir / "README.md")
        
        # Variant cards
        variants = config.get('variants', {})
        for variant_name, variant_config in variants.items():
            variant_card = self.generate_variant_model_card(
                variant_name=variant_name,
                variant_config=variant_config,
                output_path=str(output_dir / f"README-{variant_name}.md")
            )
            cards_created[variant_name] = str(output_dir / f"README-{variant_name}.md")
        
        # Release-specific cards
        releases = config.get('releases', {})
        for release_name, release_config in releases.items():
            release_card = self.generate_release_model_card(
                release_info=release_config,
                output_path=str(output_dir / f"README-{release_name}.md")
            )
            cards_created[f'release-{release_name}'] = str(output_dir / f"README-{release_name}.md")
        
        result = {
            'action': 'create_deployment_cards',
            'status': 'success',
            'cards_created': cards_created,
            'total_cards': len(cards_created)
        }
        
        logger.info(f"Created {len(cards_created)} model cards")
        return result
    
    def update_model_card_from_config(self, config_path: str, output_path: Optional[str] = None) -> str:
        """Update model card from YAML/JSON configuration"""
        logger.info(f"Loading configuration from {config_path}")
        
        with open(config_path) as f:
            if config_path.endswith('.yaml') or config_path.endswith('.yml'):
                config = yaml.safe_load(f)
            else:
                config = json.load(f)
        
        # Generate card based on config type
        if 'variant_name' in config:
            card_content = self.generate_variant_model_card(
                variant_name=config['variant_name'],
                variant_config=config,
                output_path=output_path
            )
        elif 'version' in config:
            card_content = self.generate_release_model_card(
                release_info=config,
                output_path=output_path
            )
        else:
            card_content = self.generate_complete_model_card(
                config=config,
                output_path=output_path
            )
        
        return card_content
    
    def _format_release_highlights(self, release_info: Dict) -> str:
        """Format release highlights for display"""
        highlights = release_info.get('highlights', [])
        if not highlights:
            return "- Model performance improvements\n- Enhanced documentation\n- Quality optimizations"
        
        return "\n".join(f"- {highlight}" for highlight in highlights)
    
    def _get_target_platform(self, variant_name: str) -> str:
        """Get target platform for variant"""
        platforms = {
            'int8': "Universal (memory-optimized)",
            'int4': "Universal (ultra-low memory)",
            'onnx': "Cross-platform (accelerated)",
            'tensorrt': "NVIDIA GPUs (maximum speed)",
            'coreml': "Apple devices (iOS/macOS)",
            'tflite': "Android/Edge devices",
            'gguf': "CPU-only environments"
        }
        return platforms.get(variant_name, "Universal")
    
    def _get_memory_efficiency(self, variant_name: str) -> str:
        """Get memory efficiency description"""
        efficiencies = {
            'int8': "~50% reduction",
            'int4': "~75% reduction", 
            'onnx': "Variable by hardware",
            'tensorrt': "Hardware-dependent",
            'coreml': "Optimized for Apple Silicon",
            'tflite': "Edge-optimized",
            'gguf': "~80% reduction"
        }
        return efficiencies.get(variant_name, "Optimized")
    
    def _get_speed_characteristics(self, variant_name: str) -> str:
        """Get speed characteristics description"""
        speeds = {
            'int8': "Slightly slower than base",
            'int4': "Moderate slowdown",
            'onnx': "Accelerated on supported hardware",
            'tensorrt': "Maximum inference speed",
            'coreml': "Optimized for Apple devices",
            'tflite': "Optimized for mobile/edge",
            'gguf': "CPU-optimized inference"
        }
        return speeds.get(variant_name, "Optimized performance")
    
    def _get_quality_impact(self, variant_name: str) -> str:
        """Get quality impact description"""
        impacts = {
            'int8': "Minimal quality loss",
            'int4': "Minor quality trade-off",
            'onnx': "Equivalent quality",
            'tensorrt': "Equivalent quality",
            'coreml': "Equivalent quality",
            'tflite': "Minimal impact",
            'gguf': "Minor quality considerations"
        }
        return impacts.get(variant_name, "High quality maintained")
    
    def _get_hardware_requirements(self, variant_name: str) -> str:
        """Get hardware requirements for variant"""
        requirements = {
            'int8': "Compatible with most modern hardware",
            'int4': "Works on CPU and GPU",
            'onnx': "Requires ONNX-compatible hardware",
            'tensorrt': "NVIDIA GPU with TensorRT support",
            'coreml': "Apple devices with Core ML support",
            'tflite': "Mobile/edge devices",
            'gguf': "Any CPU-capable system"
        }
        return requirements.get(variant_name, "Standard hardware compatible")
    
    def _get_optimization_details(self, variant_name: str) -> Dict:
        """Get detailed optimization information for variant"""
        details = {
            'int8': {
                'description': "8-bit integer quantization reduces model memory usage by approximately 50% while maintaining high-quality outputs.",
                'specifications': [
                    "Quantization method: Dynamic range quantization",
                    "Weight precision: 8-bit integers",
                    "Activation precision: 8-bit integers",
                    "Calibration: Minimal calibration data required",
                    "Supported operations: Most transformer operations"
                ],
                'benefits': [
                    "Significant memory reduction",
                    "Maintained inference quality",
                    "Faster model loading",
                    "Reduced bandwidth requirements",
                    "Compatible with CPU and GPU"
                ],
                'considerations': [
                    "Slightly slower than FP16 on some operations",
                    "May require special hardware for optimal performance",
                    "Some numerical precision may be lost"
                ]
            },
            'int4': {
                'description': "4-bit quantization provides maximum memory efficiency with acceptable quality preservation for most use cases.",
                'specifications': [
                    "Quantization method: NF4 (NormalFloat4)",
                    "Weight precision: 4-bit integers",
                    "Double quantization: Enabled for better precision",
                    "Compute dtype: bfloat16",
                    "Group quantization: Optimal grouping strategy"
                ],
                'benefits': [
                    "Maximum memory efficiency (~75% reduction)",
                    "Enables deployment on resource-constrained devices",
                    "Faster inference on CPU",
                    "Reduced power consumption",
                    "Smaller download size"
                ],
                'considerations': [
                    "Noticeable quality trade-off on complex tasks",
                    "May require more careful prompt engineering",
                    "Best suited for inference, not training",
                    "Performance varies by hardware"
                ]
            }
        }
        
        return details.get(variant_name, {
            'description': f"Optimized {variant_name} version with specific performance characteristics.",
            'specifications': ["Optimization details not available"],
            'benefits': ["Performance improvements"],
            'considerations': ["Review requirements for your use case"]
        })
    
    def _get_variant_use_cases(self, variant_name: str) -> str:
        """Get use cases for variant"""
        use_cases = {
            'int8': """**Best for:**
- Production deployment with memory constraints
- Development and testing environments
- Cloud inference services
- Systems with limited GPU memory""",
            
            'int4': """**Best for:**
- Edge devices and mobile deployment
- CPU-only inference
- Applications with strict memory limits
- Resource-constrained environments""",
            
            'onnx': """**Best for:**
- Cross-platform deployment
- Hardware-accelerated inference
- Enterprise applications
- Systems requiring consistent performance"""
        }
        
        return use_cases.get(variant_name, "Universal deployment scenarios")
    
    def _get_variant_specific_examples(self, variant_name: str) -> str:
        """Get variant-specific usage examples"""
        examples = {
            'int8': """
```python
# Load 8-bit quantized model
from transformers import BitsAndBytesConfig

quantization_config = BitsAndBytesConfig(
    load_in_8bit=True,
    llm_int8_threshold=6.0,
)

model = AutoModelForCausalLM.from_pretrained(
    "sheikh-team/sheikh-25-coder-int8",
    quantization_config=quantization_config,
    device_map="auto"
)
```""",
            
            'int4': """
```python
# Load 4-bit quantized model
from transformers import BitsAndBytesConfig
import torch

quantization_config = BitsAndBytesConfig(
    load_in_4bit=True,
    bnb_4bit_compute_dtype=torch.bfloat16,
    bnb_4bit_use_double_quant=True,
    bnb_4bit_quant_type="nf4"
)

model = AutoModelForCausalLM.from_pretrained(
    "sheikh-team/sheikh-25-coder-int4",
    quantization_config=quantization_config,
    device_map="auto"
)
```""",
            
            'onnx': """
```python
# Load ONNX optimized model
from optimum.onnxruntime import ORTModelForCausalLM
from transformers import AutoTokenizer

model = ORTModelForCausalLM.from_pretrained(
    "sheikh-team/sheikh-25-coder-onnx",
    export=True
)

tokenizer = AutoTokenizer.from_pretrained(
    "sheikh-team/sheikh-25-coder-onnx"
)
```
"""
        }
        
        return examples.get(variant_name, "See general usage examples above.")


def main():
    """Main function for model card generation"""
    parser = argparse.ArgumentParser(description='Generate model cards')
    
    parser.add_argument('--output_path', help='Output path for model card')
    parser.add_argument('--config', help='Configuration file (YAML/JSON)')
    parser.add_argument('--variant', help='Variant name for variant card')
    parser.add_argument('--release', help='Release version for release card')
    parser.add_argument('--model_name', default='Sheikh-2.5-Coder', help='Model name')
    parser.add_argument('--base_model', default='microsoft/phi-2', help='Base model name')
    parser.add_argument('--action', required=True,
                       choices=['generate', 'update', 'deployment_cards'],
                       help='Action to perform')
    
    # Configuration options
    parser.add_argument('--model_size', help='Model size in parameters')
    parser.add_argument('--context_length', help='Context length')
    parser.add_argument('--quantization', help='Quantization type')
    parser.add_argument('--learning_rate', help='Learning rate')
    parser.add_argument('--batch_size', help='Batch size')
    
    args = parser.parse_args()
    
    # Initialize generator
    generator = ModelCardGenerator(
        model_name=args.model_name,
        base_model=args.base_model
    )
    
    # Build config from arguments
    config = {
        'model_size': args.model_size,
        'context_length': args.context_length,
        'quantization': args.quantization,
        'learning_rate': args.learning_rate,
        'batch_size': args.batch_size
    }
    
    # Remove None values
    config = {k: v for k, v in config.items() if v is not None}
    
    # Execute action
    if args.action == 'generate':
        if args.variant:
            result = generator.generate_variant_model_card(
                variant_name=args.variant,
                variant_config=config,
                output_path=args.output_path
            )
        elif args.release:
            release_info = config.copy()
            release_info['version'] = args.release
            result = generator.generate_release_model_card(
                release_info=release_info,
                output_path=args.output_path
            )
        else:
            result = generator.generate_complete_model_card(
                config=config,
                output_path=args.output_path
            )
        
        print(f"Model card generated successfully!")
        print(f"Content length: {len(result)} characters")
        
    elif args.action == 'update':
        if not args.config:
            print("Error: --config required for update action")
            return 1
        
        result = generator.update_model_card_from_config(
            config_path=args.config,
            output_path=args.output_path
        )
        print(f"Model card updated from configuration!")
        
    elif args.action == 'deployment_cards':
        if not args.config:
            print("Error: --config required for deployment_cards action")
            return 1
        
        with open(args.config) as f:
            if args.config.endswith('.yaml') or args.config.endswith('.yml'):
                full_config = yaml.safe_load(f)
            else:
                full_config = json.load(f)
        
        result = generator.create_deployment_cards(
            config=full_config,
            output_dir=args.output_path or "model_cards"
        )
        
        print(json.dumps(result, indent=2, default=str))
    
    else:
        print(f"Unknown action: {args.action}")
        return 1
    
    return 0


if __name__ == '__main__':
    sys.exit(main())