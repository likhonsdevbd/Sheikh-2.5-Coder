# Sheikh-2.5-Coder Model Optimization Configuration
# Comprehensive configuration for on-device deployment optimization

model_config:
  model_name: "Sheikh-2.5-Coder"
  architecture: "minimax_m2"
  total_parameters: "3.09B"
  base_hidden_size: 2048
  base_intermediate_size: 8192
  num_layers: 36
  base_num_attention_heads: 16
  base_num_key_value_heads: 2
  vocab_size: 32000
  max_position_embeddings: 32768

# Quantization Configuration
quantization:
  # INT8 Quantization
  int8:
    enabled: true
    method: "dynamic"  # dynamic, static, weight_only
    weights_only: false
    activations: true
    calibrate: true
    calibration_steps: 100
    fallback_precision: "fp16"
    
  # INT4 Quantization
  int4:
    enabled: true
    method: "nf4"  # nf4, fp4, weight_only
    use_gptq: true
    group_size: 64
    desc_act: false
    fallback_precision: "int8"
    
  # Mixed Precision
  mixed_precision:
    enabled: true
    weights: "fp16"
    activations: "fp16"
    compute_dtype: "fp16"
    allow_tf32: true

# Memory Optimization
memory_optimization:
  # Model Pruning
  pruning:
    enabled: true
    method: "magnitude"  # magnitude, structured, lottery_ticket
    sparsity_level: 0.3  # 30% sparsity
    prune_neurons: true
    prune_attention_heads: true
    minimal_remaining_heads: 4
    
  # Attention Head Optimization
  attention_optimization:
    reduce_q_heads: false  # Only reduce if memory constrained
    target_heads: 8
    importance_threshold: 0.1
    preserve_kv_heads: true
    
  # Layer Fusion
  layer_fusion:
    enabled: true
    fuse_qkv: true
    fuse_attention_output: true
    fuse_mlp: true
    
  # KV Cache Optimization
  kv_cache:
    enabled: true
    compression: "fp16"
    sliding_window: 4096
    eviction_policy: "lru"

# Inference Acceleration
acceleration:
  # ONNX Export
  onnx:
    enabled: true
    opset_version: 17
    optimize_for_inference: true
    fuse_gelu: true
    fuse_layernorm: true
    export_qkv: true
    
  # TensorRT
  tensorrt:
    enabled: true
    precision: "fp16"  # fp32, fp16, int8, mixed
    dynamic_batching: true
    max_workspace_size: "8GB"
    builder_optimization_level: 3
    
  # OpenVINO
  openvino:
    enabled: true
    precision: "fp16"
    optimization_level: 3
    num_threads: 8
    
  # TorchScript
  torchscript:
    enabled: true
    optimize: true
    strict_type_checks: false
    
  # Flash Attention
  flash_attention:
    enabled: true
    implementation: "flash_attn"  # flash_attn, xformers, native
    memory_efficient: true

# Deployment Targets
deployment_targets:
  mobile:
    # 6-8GB RAM
    max_memory_gb: 8
    quantization: "int4"
    context_length: 4096
    batch_size: 1
    threads: 4
    optimization_level: "O3"
    compression_ratio: 0.6
    
  edge:
    # 8-12GB RAM
    max_memory_gb: 12
    quantization: "int8"
    context_length: 8192
    batch_size: 2
    threads: 6
    optimization_level: "O3"
    compression_ratio: 0.8
    
  desktop:
    # 12-16GB RAM
    max_memory_gb: 16
    quantization: "fp16"
    context_length: 16384
    batch_size: 4
    threads: 8
    optimization_level: "O3"
    compression_ratio: 0.9
    
  server:
    # 16GB+ RAM
    max_memory_gb: 32
    quantization: "fp32"
    context_length: 32768
    batch_size: 8
    threads: 16
    optimization_level: "O3"
    compression_ratio: 1.0

# Benchmarking Configuration
benchmarking:
  metrics:
    - "memory_footprint"
    - "inference_speed"
    - "tokens_per_second"
    - "latency_p50"
    - "latency_p95"
    - "quality_score"
    - "battery_impact"
    
  test_cases:
    - prompt_length: 256
      context_length: 1024
      batch_size: 1
    - prompt_length: 512
      context_length: 2048
      batch_size: 1
    - prompt_length: 1024
      context_length: 4096
      batch_size: 2
    
  quality_evaluation:
    codebleu: true
    pass_k: true
    human_evaluation: false
    custom_metrics:
      - "code_completion_accuracy"
      - "syntax_correctness"

# Validation Requirements
validation:
  functional_correctness: true
  performance_impact: true
  quality_preservation: true
  memory_efficiency: true
  deployment_compatibility: true
  
  tolerance_thresholds:
    performance_degradation: 0.05  # 5% maximum degradation
    quality_degradation: 0.02     # 2% maximum quality loss
    memory_reduction: 0.3          # 30% memory reduction target

# Optimization Pipeline
optimization_pipeline:
  sequence:
    - "quantization"
    - "pruning"
    - "layer_fusion"
    - "export_optimization"
    - "inference_acceleration"
    
  parallel_optimizations:
    - "memory_optimization"
    - "acceleration_setup"
    
  validation_steps:
    - "functional_test"
    - "performance_benchmark"
    - "quality_evaluation"

# Logging and Monitoring
logging:
  level: "INFO"
  log_optimization_steps: true
  save_intermediate_results: true
  tensorboard_logging: true
  profile_memory_usage: true
  
# Hardware Compatibility
hardware_requirements:
  gpu:
    min_memory_gb: 6
    recommended_gb: 8
    compute_capability: "7.0"
    
  cpu:
    min_cores: 4
    recommended_cores: 8
    instruction_sets: ["avx2", "fma"]
    
  ram:
    min_gb: 8
    recommended_gb: 16
    ddr_generation: 4

# Output Configuration
output:
  save_optimized_models: true
  model_format: "safetensors"
  include_metadata: true
  compression: true
  
  paths:
    optimized_models: "models/optimized"
    benchmarks: "benchmarks"
    logs: "logs/optimization"
    cache: "cache/optimization"