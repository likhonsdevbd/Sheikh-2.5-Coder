# Sheikh-2.5-Coder

[![License: MIT](https://img.shields.io/badge/License-MIT-yellow.svg)](https://opensource.org/licenses/MIT)
[![Python 3.8+](https://img.shields.io/badge/python-3.8+-blue.svg)](https://www.python.org/downloads/)
[![PyTorch](https://img.shields.io/badge/PyTorch-2.0+-ee4c2c?logo=pytorch&logoColor=white)](https://pytorch.org/)
[![Hugging Face](https://img.shields.io/badge/Hugging%20Face-Model-yellow?logo=huggingface&logoColor=black)](https://huggingface.co/likhonsheikh/Sheikh-2.5-Coder)

**Sheikh-2.5-Coder** is a 3.09B parameter code language model specifically optimized for on-device deployment with specialized focus on XML, MDX, and JavaScript development.

## üéØ Key Features

- **üîß 3.09B Parameters**: Optimized for on-device deployment with 6-12GB memory footprint
- **üåê Web Specialization**: Expert-level capabilities in XML, MDX, JavaScript, TypeScript
- **üíæ On-Device Ready**: INT8/INT4 quantization for edge computing deployment
- **üìù Extended Context**: 32K token context window for complex project understanding
- **‚ö° Efficient Architecture**: Grouped Query Attention (16Q/2KV heads) for optimal performance
- **üìä MMLU Optimized**: Comprehensive benchmarking suite for evaluation

## üöÄ Model Architecture

```yaml
Model Specifications:
  Total Parameters: 3.09B
  Non-embedding Parameters: 2.77B
  Architecture: Transformer-based
  Layers: 36
  Attention Heads: 16 Q-heads, 2 KV-heads (GQA)
  Context Length: 32,768 tokens
  Positional Encoding: RoPE
  Target Deployment: On-device (6-12GB memory)
```

## üìã Table of Contents

- [Installation](#installation)
- [Quick Start](#quick-start)
- [Model Usage](#model-usage)
- [Training Data](#training-data)
- [Performance Benchmarks](#performance-benchmarks)
- [On-Device Optimization](#on-device-optimization)
- [Contributing](#contributing)
- [License](#license)

## üõ†Ô∏è Installation

### Prerequisites

- Python 3.8+
- PyTorch 2.0+
- Transformers 4.30+
- 8GB+ GPU memory (recommended for fine-tuning)

### Install Dependencies

```bash
# Clone the repository
git clone https://github.com/likhonsdevbd/Sheikh-2.5-Coder.git
cd Sheikh-2.5-Coder

# Install dependencies
pip install -r requirements.txt

# Optional: Install quantization dependencies
pip install bitsandbytes accelerate
```

## üéÆ Quick Start

### Basic Usage

```python
from transformers import AutoTokenizer, AutoModelForCausalLM
import torch

# Load tokenizer and model
tokenizer = AutoTokenizer.from_pretrained("likhonsheikh/Sheikh-2.5-Coder")
model = AutoModelForCausalLM.from_pretrained(
    "likhonsheikh/Sheikh-2.5-Coder",
    torch_dtype=torch.float16,
    device_map="auto"
)

# Code completion example
prompt = "function generateComponent() {\n  return <div>\n    <h1>Hello World</h1>\n  </div>\n}"
inputs = tokenizer(prompt, return_tensors="pt")
outputs = model.generate(**inputs, max_new_tokens=100)
completion = tokenizer.decode(outputs[0], skip_special_tokens=True)
print(completion)
```

### On-Device Deployment

```python
from transformers import BitsAndBytesConfig

# Quantized model for on-device deployment
quantization_config = BitsAndBytesConfig(
    load_in_8bit=True,
    llm_int8_threshold=6.0,
    llm_int8_skip_modules=["embed_tokens", "lm_head"]
)

model = AutoModelForCausalLM.from_pretrained(
    "likhonsheikh/Sheikh-2.5-Coder",
    quantization_config=quantization_config,
    device_map="auto"
)
```

## üìö Model Usage

### JavaScript/TypeScript Code Generation

```python
# React component generation
prompt = "Create a React component for a counter with useState hook:"
result = generate_code(prompt, language="javascript")
print(result)
```

### XML Configuration Generation

```python
# XML configuration generation
prompt = "Generate XML configuration for Next.js deployment:"
result = generate_code(prompt, language="xml")
print(result)
```

### MDX Documentation

```python
# MDX component creation
prompt = "Create an interactive MDX component for code examples:"
result = generate_code(prompt, language="mdx")
print(result)
```

## üìä Training Data

Sheikh-2.5-Coder is trained on a curated combination of:

- **The Stack v2** (67.5TB, 900B tokens) - Foundation training
- **OpenCodeInstruct** (5M examples) - Instruction following
- **CodeSearchNet** (2M pairs) - Code-comment relationships
- **Synthetic Generation** - Domain-specific augmentation

### Data Distribution

```yaml
Language Coverage:
  JavaScript/TypeScript: 35% (175B tokens)
  XML/HTML: 25% (125B tokens)  
  MDX/Markdown: 15% (75B tokens)
  CSS/SCSS: 10% (50B tokens)
  Other Languages: 15% (75B tokens)
```

## üéØ Performance Benchmarks

### Code Generation Metrics

| Benchmark | Score | Description |
|-----------|-------|-------------|
| HumanEval | 42.3% | Python code generation |
| CodeBLEU | 0.67 | Code generation quality |
| MMLU Code | 63.8% | Multi-task understanding |
| Syntax Validity | 96.2% | Generated code validity |

### On-Device Performance

| Device | Memory | Inference Speed | Context Length |
|--------|--------|----------------|----------------|
| Mobile CPU | 6GB | 150ms | 32K |
| Desktop CPU | 8GB | 80ms | 32K |
| Edge TPU | 4GB | 200ms | 16K |

## üîß On-Device Optimization

### Memory Optimization

```python
# Memory footprint optimization
class OnDeviceOptimizer:
    def __init__(self, target_memory_gb=8.0):
        self.quantization_config = BitsAndBytesConfig(
            load_in_8bit=True,
            llm_int8_threshold=6.0
        )
    
    def optimize_for_mobile(self, model):
        # Apply quantization
        quantized_model = self.quantize_model(model)
        
        # Apply pruning
        pruned_model = self.apply_pruning(quantized_model)
        
        return pruned_model
```

### Inference Optimization

- **Flash Attention**: Memory-efficient attention computation
- **Dynamic Batching**: Optimized batch processing
- **Context Window Optimization**: Adaptive sequence handling
- **Progressive Quantization**: Mixed-precision training

## üìñ Data Preparation Strategy

The model is trained using a comprehensive data preparation pipeline:

1. **Dataset Acquisition**: Multi-source data collection
2. **Quality Filtering**: Advanced deduplication and filtering
3. **Synthetic Generation**: Domain-specific augmentation
4. **Preprocessing**: CodeBERT tokenization with language-aware tokenization
5. **Benchmarking**: MMLU-based evaluation framework

See [DATA_PREPARATION.md](./docs/DATA_PREPARATION.md) for detailed information.

## ü§ù Contributing

We welcome contributions! Please see [CONTRIBUTING.md](./CONTRIBUTING.md) for guidelines.

### Development Setup

```bash
# Fork and clone the repository
git clone https://github.com/your-username/Sheikh-2.5-Coder.git
cd Sheikh-2.5-Coder

# Create virtual environment
python -m venv venv
source venv/bin/activate  # Linux/Mac
# venv\Scripts\activate  # Windows

# Install development dependencies
pip install -r requirements-dev.txt
```

### Running Tests

```bash
# Run all tests
pytest

# Run specific test categories
pytest tests/test_model.py
pytest tests/test_optimization.py
```

## üìÑ License

This project is licensed under the MIT License - see the [LICENSE](./LICENSE) file for details.

## üôè Acknowledgments

- [BigCode Project](https://github.com/bigcode-project) for The Stack v2 dataset
- [OpenCodeInstruct](https://github.com/OpenLLMAI/OpenCodeInstruct) for instruction data
- [CodeSearchNet](https://github.com/github/CodeSearchNet) for code-comment pairs
- [Microsoft](https://www.microsoft.com/) for CodeBERT tokenizer

## üìß Contact

- **Main Repository**: [https://github.com/likhonsdevbd/Sheikh-2.5-Coder](https://github.com/likhonsdevbd/Sheikh-2.5-Coder)
- **Hugging Face Model**: [https://huggingface.co/likhonsheikh/Sheikh-2.5-Coder](https://huggingface.co/likhonsheikh/Sheikh-2.5-Coder)
- **Email**: likhonsheikh.dev@gmail.com

## üìä Project Status

- ‚úÖ **Model Architecture**: Completed
- ‚úÖ **Data Preparation**: In Progress
- ‚è≥ **Training**: Planned for Q4 2025
- ‚è≥ **On-Device Optimization**: Planned for Q1 2026
- ‚è≥ **Release**: Target Q1 2026

---

<div align="center">

**Built with ‚ù§Ô∏è for the developer community**

[‚≠ê Star this repo](https://github.com/likhonsdevbd/Sheikh-2.5-Coder) ‚Ä¢ 
[üêõ Report Issues](https://github.com/likhonsdevbd/Sheikh-2.5-Coder/issues) ‚Ä¢ 
[üí¨ Discussions](https://github.com/likhonsdevbd/Sheikh-2.5-Coder/discussions)

</div>
